{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Auto encoders with MNIST data\n",
    "\n",
    "https://www.kaggle.com/rvislaywade/visualizing-mnist-using-a-variational-autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contents**\n",
    "1. Introduction\n",
    "    - What is autoencoding\n",
    "    - Autoencoders\n",
    "    - The variatonal variety\n",
    "    \n",
    "2. Data preparation\n",
    "   - Load Data\n",
    "   - Combine train and test\n",
    "   - Split into new training and validation sets\n",
    "   - Reshape and Normmalize\n",
    "  \n",
    "3. Model construction\n",
    "    - Encoder Network\n",
    "    - Sampling function\n",
    "    - decoder Network\n",
    "    - Loss\n",
    " \n",
    "4. Train the VAE\n",
    "\n",
    "5. Results\n",
    "    - Clusteering of digits in latent space\n",
    "    - Sample digits\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Variational auto encoders can be used to visualize high dimensional dat in a meaningful lower dimensional space. \n",
    "\n",
    "#### A. What is Auto encoding?\n",
    "\n",
    "Autoencoding is much like what sounds in the sense that the input and output are essentially the same. Th autoencoders can be used as preprocessing the data before training a supervised learning algorithm.\n",
    "\n",
    "#### B. Autoencoders\n",
    "\n",
    "Generally autoencoders consits of three main parts: an encoder, a decoder.\n",
    "\n",
    "#### C. Variational Variety\n",
    "\n",
    "There are different kind of autoencoders. Variational auroencoders not oly learn to morph the data in and out of a compressed representation of itself like the vanila auto encoders. Instead they learn the probability distribution that the data come from. Since they learn about the distribution the inputs came from, we can sample from that distribution to generate novel data. VAEs can also be used to cluster data in useful ways\n",
    "\n",
    "<img src='https://i.imgur.com/ZN6MyTx.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the differenc between a simple autoencoder and a Variational autoencoder?**\n",
    "https://jamesmccaffrey.wordpress.com/2018/07/03/the-difference-between-an-autoencoder-and-a-variational-autoencoder/\n",
    "\n",
    "**simple Autoencoder**\n",
    "An autoencoder takes the input, compress it, and then recreates the original input. This is an unsupervised techinique because we are not using any labels but only the original data. The two main uses of autoencoders are 1. To compress datato two or three dimensions so it  can be graphed, 2. To compress and decompress images or documents that removes the noise.\n",
    "\n",
    "**Variational Autoencoder**\n",
    "A variational autoencoder assumes that the sourse data has some sort of underlying probability distribution (such as gaussian distribution) and then attempts to find the parameter of the distribution. Implementing VAE is very challenging than simple AEw. The main use use of VAE is to generate new data that is related to the original data. Now what the additional data is good for is hard to say. A VAE is a generative system  and serves as a similar purpose as a Generative Adversarial Neetwork although GANs work quite differently.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras import metrics\n",
    "from keras import backend as K # generic backgound so code works with \n",
    "                                 # tensorflow or theano\n",
    "np.random.seed(237)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train),(X_valid,y_valid) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize and resize the images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change  datatype to float\n",
    "X_train = X_train.astype('float32')\n",
    "X_valid = X_valid.astype('float32')\n",
    "\n",
    "#rescale the values btw '0-1'\n",
    "X_train/=255\n",
    "X_valid/=255\n",
    "\n",
    "X_train = X_train.reshape(-1,28,28,1)\n",
    "X_valid = X_valid.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_train[13][:,:,0])\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[690][:,:,0])\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_train[2375][:,:,0])\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_train[42013][:,:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAE has three basic parts\n",
    "\n",
    "1. Encoder: that learns the parameters (mean and the varaince) of the underlying low dimensional latent distribution of the original high dimensional Input Image\n",
    "\n",
    "2. A sampling function tht samples from the distribution\n",
    "\n",
    "3. A decoder that can turn the sample data from the learned distribution and coverts it to highdimensional Output data\n",
    "\n",
    "In this example both the encoder and the decoder are CNNs.\n",
    "\n",
    "####  A. Encodr Network\n",
    "\n",
    "The encoder has two output layers, one for the latent distribution mean and teh other for its variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (28,28,1)\n",
    "batch_size = 16\n",
    "latent_dim = 2 # number of latent ddimensions\n",
    "\n",
    "# encoder Architeccture:\n",
    "#Input >> Conv2d*4 >> Flatten >> Dense\n",
    "input_img = keras.Input(shape=img_shape)\n",
    "\n",
    "x = layers.Conv2D(32,3,padding='same',activation='relu')(input_img)\n",
    "x = layers.Conv2D(64,3,padding='same',activation='relu',strides=(2,2))(x)\n",
    "x = layers.Conv2D(64,3,padding='same',activation='relu')(x)\n",
    "x = layers.Conv2D(64,3,padding='same',activation='relu')(x)\n",
    "\n",
    "# need to know the shape of the network here for the decoder \n",
    "shape_before_flattening = K.int_shape(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(32,activation='relu')(x)\n",
    "\n",
    "# two outputs: Latent mean and (log) variance\n",
    "z_mu = layers.Dense(2)(x)\n",
    "z_log_sigma = layers.Dense(2)(x) #note that the inut is 'x' but not 'z_mu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Sampling Function\n",
    "\n",
    "This function samples the data from the sampling distribution learned by the encoder (the parameters learned: z_mu, z_log_sigma). $epsilon$ is a tensor of small random normal values. One of the assumptions underlying a VAE is that the data came from random normal distribution in latent space. \n",
    "\n",
    "With keras everything has to be in a layer format to compile correctly. This goes for the sampling function too. The lambda wrapper can be used for this operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling function\n",
    "def sampling(args):\n",
    "    \n",
    "    z_mu, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mu)[0], latent_dim),\n",
    "                             mean=0., stddev=1.)\n",
    "    return z_mu + K.exp(z_log_sigma)*epsilon\n",
    "\n",
    "# sample vector from the latent distribution\n",
    "z = layers.Lambda(sampling)([z_mu, z_log_sigma])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Decoder Network\n",
    "the deoder is baically the inverse of the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder takes the latent distribution sample 'Z' as input \n",
    "\n",
    "decoder_input = layers.Input(K.int_shape(z)[1:])\n",
    "\n",
    "#expanding to 784 pixels\n",
    "x = layers.Dense(np.prod(shape_before_flattening[1:]),\n",
    "                activation='relu')(decoder_input)\n",
    "\n",
    "#reshape\n",
    "x = layers.Reshape(shape_before_flattening[1:])(x)\n",
    "\n",
    "# use Conv2DTranspose to reverse the cov layers from the encoder\n",
    "x = layers.Conv2DTranspose(32,3,padding='same',activation='relu',\n",
    "                          strides=(2,2))(x)\n",
    "\n",
    "x = layers.Conv2D(1,3,padding='same',activation='sigmoid')(x)\n",
    "\n",
    "#decoder model statement\n",
    "decoder = Model(decoder_input,x)\n",
    "\n",
    "# apply the decoder to the sample from the latent space\n",
    "z_decoded = decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Loss function\n",
    "\n",
    "VAE requires a unique loss function. VAE is trained using a loss function with two components\n",
    "\n",
    "1. **Reconstruction Loss**: This is the cross-entropy describing the errors between the decoded samples from the latent distribution and the original inputs.\n",
    "\n",
    "2. **The KL-divergence**: it is between the latent distribution and the prior (this acts a sort of regulaization term)\n",
    "\n",
    "We define a custom layer class that calculates the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0330 00:15:45.546758 11008 deprecation_wrapper.py:119] From C:\\Users\\Navyanth\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0330 00:15:45.562785 11008 deprecation.py:323] From C:\\Users\\Navyanth\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# construct a custom layer to calculate the loss\n",
    "class CustomVariationalLayer(keras.layers.Layer):\n",
    "    \n",
    "    def vae_loss(self, x, z_decoded):\n",
    "        #x is input (true) and z_decoded is reconst image (predicted) \n",
    "        x = K.flatten(x)\n",
    "        z_decoded = K.flatten(z_decoded)\n",
    "        \n",
    "        #reconstruction loss\n",
    "        xent_loss = keras.metrics.binary_crossentropy(x,z_decoded)\n",
    "        \n",
    "        # KL divergence\n",
    "        kl_loss = -5e-4*K.mean(1+z_log_sigma - K.square(z_mu)-K.exp(z_log_sigma),axis=-1)\n",
    "        \n",
    "        return K.mean(xent_loss+kl_loss)\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        \n",
    "        x = inputs[0]           #actual image\n",
    "        z_decoded = inputs[1]   # predicted image\n",
    "        my_loss = self.vae_loss(x,z_decoded)\n",
    "        \n",
    "        # define the loss function to the layer\n",
    "        self.add_loss(my_loss, inputs=inputs) \n",
    "        \n",
    "        return x\n",
    "        \n",
    "# apply the custom loss to the  input images and the decoded\n",
    "# latent distribution\n",
    "    \n",
    "y = CustomVariationalLayer()([input_img, z_decoded])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instatiate teh model and take a look at the summry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model statement\n",
    "vae = Model(input_img, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 28, 28, 32)   320         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 14, 14, 64)   18496       conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 14, 14, 64)   36928       conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 14, 14, 64)   36928       conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 12544)        0           conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 32)           401440      flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 2)            66          dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 2)            66          dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 2)            0           dense_18[0][0]                   \n",
      "                                                                 dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 28, 28, 1)    56385       lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "custom_variational_layer_1 (Cus [(None, 28, 28, 1),  0           input_11[0][0]                   \n",
      "                                                                 model_1[1][0]                    \n",
      "==================================================================================================\n",
      "Total params: 550,629\n",
      "Trainable params: 550,629\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 12544)             37632     \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 28, 28, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 28, 28, 1)         289       \n",
      "=================================================================\n",
      "Total params: 56,385\n",
      "Trainable params: 56,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.fit(x=X_train, y=None, shuffle=True, epochs=7,\n",
    "       batch_size=batch_size, validation_data=(X_valid,None))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
